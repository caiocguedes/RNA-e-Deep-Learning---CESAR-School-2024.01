{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/vcasadei/Redes-Neurais-CESAR-School/blob/main/3%20-%20Regress%C3%A3o%20Log%C3%ADstica/3.2-LogisticRegressionMNIST.ipynb","timestamp":1732827292274}],"gpuType":"V28"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Ot79tGiPOl08"},"source":["# Regressão Softmax com dados do MNIST"]},{"cell_type":"markdown","metadata":{"id":"21fecJPyOl0-"},"source":["## Objetivo"]},{"cell_type":"markdown","metadata":{"id":"z8SxRSoLOl1I"},"source":["O objetivo deste notebook é ilustrar o uso de praticamente a mesma rede desenvolvida para a classificação das flores Íris, porém agora com o problema de classificação de dígitos manuscritos utilizando o dataset MNIST.\n","As principais diferenças são:\n","- tipo do dado, agora imagem com muito atributos: 28 x 28 pixels\n","- número de amostras, muito maior, 60 mil\n","Neste exercício será possível a interpretação do significado dos parâmetros treinados"]},{"cell_type":"markdown","metadata":{"id":"RKVT1A6zOl1J"},"source":["## Importação das bibliotecas"]},{"cell_type":"code","source":["!pip install Pillow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f5JX-6e7nYVK","executionInfo":{"status":"ok","timestamp":1732828481000,"user_tz":180,"elapsed":1786,"user":{"displayName":"CAIO CÉSAR SILVA GUEDES","userId":"04767638617673838831"}},"outputId":"d11529a7-5603-42eb-f3d8-ffc9baa901e8"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":511},"id":"wzpZj0m09RYQ","executionInfo":{"status":"error","timestamp":1732828487458,"user_tz":180,"elapsed":257,"user":{"displayName":"CAIO CÉSAR SILVA GUEDES","userId":"04767638617673838831"}},"outputId":"cab9eca6-70a1-4bfd-81cc-dc42ab8286dd"},"source":["%matplotlib inline\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","import torch\n","from torch.autograd import Variable\n","\n","import torchvision"],"execution_count":20,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"cannot import name 'PILLOW_VERSION' from 'PIL' (/usr/local/lib/python3.10/dist-packages/PIL/__init__.py)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-6ba6a81a5f26>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msvhn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVHN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mphototour\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhotoTour\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfakedata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFakeData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msemeion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSEMEION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0momniglot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOmniglot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/fakedata.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m __all__ = [\"Compose\", \"ToTensor\", \"ToPILImage\", \"Normalize\", \"Resize\", \"Scale\", \"CenterCrop\", \"Pad\",\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageOps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageEnhance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPILLOW_VERSION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0maccimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'PILLOW_VERSION' from 'PIL' (/usr/local/lib/python3.10/dist-packages/PIL/__init__.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["! pip install Pillow"],"metadata":{"id":"38nuJEV3nGuG","executionInfo":{"status":"aborted","timestamp":1732828438166,"user_tz":180,"elapsed":11,"user":{"displayName":"CAIO CÉSAR SILVA GUEDES","userId":"04767638617673838831"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EVN-NfMTNk73","executionInfo":{"status":"aborted","timestamp":1732828438166,"user_tz":180,"elapsed":11,"user":{"displayName":"CAIO CÉSAR SILVA GUEDES","userId":"04767638617673838831"}}},"source":["! pip install torchvision==0.2.1"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -U pillow"],"metadata":{"id":"ipuuBaKWmwXj","executionInfo":{"status":"aborted","timestamp":1732828438167,"user_tz":180,"elapsed":12,"user":{"displayName":"CAIO CÉSAR SILVA GUEDES","userId":"04767638617673838831"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BmeldMPaOl1p"},"source":["## Carregamento dos dados do MNIST"]},{"cell_type":"code","metadata":{"id":"vzrUS_xbgvry","executionInfo":{"status":"aborted","timestamp":1732828438167,"user_tz":180,"elapsed":12,"user":{"displayName":"CAIO CÉSAR SILVA GUEDES","userId":"04767638617673838831"}}},"source":["! git clone https://github.com/vcasadei/MNIST.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f6tbsAECg5JB","executionInfo":{"status":"aborted","timestamp":1732828438167,"user_tz":180,"elapsed":11,"user":{"displayName":"CAIO CÉSAR SILVA GUEDES","userId":"04767638617673838831"}}},"source":["!ls MNIST"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-11-24T15:44:50.638218","start_time":"2017-11-24T15:44:50.098808"},"id":"q0KuxPGVOl1q","executionInfo":{"status":"aborted","timestamp":1732828438167,"user_tz":180,"elapsed":11,"user":{"displayName":"CAIO CÉSAR SILVA GUEDES","userId":"04767638617673838831"}}},"source":["dataset_dir = 'MNIST/'\n","\n","x_train, y_train = torch.load(dataset_dir + 'processed/training.pt')\n","\n","print(\"Amostras de treinamento:\", x_train.size(0))\n","\n","print(\"\\nDimensões dos dados das imagens:   \", x_train.size())\n","print(\"Valores mínimo e máximo dos pixels:\", torch.min(x_train), torch.max(x_train))\n","print(\"Tipo dos dados das imagens:        \", type(x_train))\n","print(\"Tipo das classes das imagens:      \", type(y_train))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DccnsPRCOl1y"},"source":["### Carregamento, normalização e seleção dos dados do MNIST\n","\n","Neste exemplo utilizaremos apenas 1000 amostras de treinamento."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-11-24T15:44:50.895668","start_time":"2017-11-24T15:44:50.640110"},"id":"jRfSlVpnOl10","executionInfo":{"status":"aborted","timestamp":1732828438167,"user_tz":180,"elapsed":11,"user":{"displayName":"CAIO CÉSAR SILVA GUEDES","userId":"04767638617673838831"}}},"source":["x_train = x_train.float()\n","\n","x_train = x_train / 255.\n","\n","if True:\n","    n_samples_train = 1000\n","\n","    x_train = x_train[:n_samples_train]\n","    y_train = y_train[:n_samples_train]\n","\n","print(\"Amostras de treinamento:\", x_train.size(0))\n","\n","print(\"\\nDimensões dos dados das imagens:   \", x_train.size())\n","print(\"Valores mínimo e máximo dos pixels:\", torch.min(x_train), torch.max(x_train))\n","print(\"Tipo dos dados das imagens:        \", type(x_train))\n","print(\"Tipo das classes das imagens:      \", type(y_train))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0tXHwvS_Ol13"},"source":["### Visualizando os dados"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-11-24T16:32:19.474568","start_time":"2017-11-24T16:32:19.207270"},"id":"zV7-8V9UOl15","executionInfo":{"status":"aborted","timestamp":1732828438167,"user_tz":180,"elapsed":11,"user":{"displayName":"CAIO CÉSAR SILVA GUEDES","userId":"04767638617673838831"}}},"source":["n_samples = 24\n","\n","# cria um grid com as imagens\n","grid = torchvision.utils.make_grid(x_train[:n_samples].unsqueeze(dim=1), pad_value=1.0, padding=1)\n","\n","plt.figure(figsize=(15, 10))\n","plt.imshow(grid.numpy().transpose(1, 2, 0))\n","plt.axis('off')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"biPfk_htOl2H"},"source":["### Visualizando uma imagem com o matplotlib"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-11-24T15:44:51.413232","start_time":"2017-11-24T15:44:51.251376"},"scrolled":true,"id":"tAs2Zy28Ol2I","executionInfo":{"status":"aborted","timestamp":1732828438167,"user_tz":180,"elapsed":11,"user":{"displayName":"CAIO CÉSAR SILVA GUEDES","userId":"04767638617673838831"}}},"source":["image  = x_train[0]\n","target = y_train[0]\n","\n","plt.imshow(image.numpy(), cmap='gray')\n","print('class:', target)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jWlWHjOvOl2R"},"source":["## Modelo"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-11-24T15:44:51.419287","start_time":"2017-11-24T15:44:51.415065"},"id":"8OeUJnUqOl2T","executionInfo":{"status":"aborted","timestamp":1732828438167,"user_tz":180,"elapsed":10,"user":{"displayName":"CAIO CÉSAR SILVA GUEDES","userId":"04767638617673838831"}}},"source":["model = torch.nn.Linear(28*28, 10) # 28*28 atributos de entrada e 10 neurônios na sáida"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"COKUqRSGOl2d"},"source":["### Testando um predict com poucas amostras"]},{"cell_type":"code","metadata":{"id":"Rxf7FUZOOl2g","executionInfo":{"status":"aborted","timestamp":1732828438167,"user_tz":180,"elapsed":10,"user":{"displayName":"CAIO CÉSAR SILVA GUEDES","userId":"04767638617673838831"}}},"source":["xin = x_train[:5].view(-1,28*28)\n","score = model(Variable(xin))\n","score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JsyINWhBOl2m"},"source":["## Treinamento"]},{"cell_type":"markdown","metadata":{"id":"Cj7WB9NzOl2n"},"source":["### Inicialização dos parâmetros"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-11-24T15:44:51.425768","start_time":"2017-11-24T15:44:51.420825"},"id":"xgTvQXGCOl2o","executionInfo":{"status":"aborted","timestamp":1732828438167,"user_tz":180,"elapsed":10,"user":{"displayName":"CAIO CÉSAR SILVA GUEDES","userId":"04767638617673838831"}}},"source":["epochs = 100\n","learningRate = 0.5\n","\n","# Utilizaremos CrossEntropyLoss como função de perda\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","# Gradiente descendente\n","optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fK50bgRzOl2z"},"source":["### Visualização do grafo computacional da perda (loss)"]},{"cell_type":"code","metadata":{"id":"sLuv_VgNENQf","executionInfo":{"status":"aborted","timestamp":1732828438168,"user_tz":180,"elapsed":11,"user":{"displayName":"CAIO CÉSAR SILVA GUEDES","userId":"04767638617673838831"}}},"source":["!pip install graphviz\n","!pip install git+https://github.com/szagoruyko/pytorchviz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-11-24T16:28:18.101867","start_time":"2017-11-24T16:28:18.062312"},"id":"icZLveYLOl20","executionInfo":{"status":"aborted","timestamp":1732828438168,"user_tz":180,"elapsed":10,"user":{"displayName":"CAIO CÉSAR SILVA GUEDES","userId":"04767638617673838831"}}},"source":["from torchviz import make_dot, make_dot_from_trace\n","y_pred = model(Variable(x_train.view(-1,28*28)))\n","loss = criterion(y_pred, Variable(y_train))\n","loss\n","# p = make_dot(loss, dict(model.named_parameters()))\n","# p"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y2GjaHFjEMTm","executionInfo":{"status":"aborted","timestamp":1732828438168,"user_tz":180,"elapsed":10,"user":{"displayName":"CAIO CÉSAR SILVA GUEDES","userId":"04767638617673838831"}}},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jnUCb6j_Ol23"},"source":["### Laço de treinamento dos pesos"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-11-24T15:44:51.678678","start_time":"2017-11-24T15:44:51.427695"},"id":"g5aavLLTOl24","executionInfo":{"status":"aborted","timestamp":1732828438168,"user_tz":180,"elapsed":10,"user":{"displayName":"CAIO CÉSAR SILVA GUEDES","userId":"04767638617673838831"}}},"source":["losses = []\n","import numpy\n","\n","import numpy\n","zs = []\n","\n","for i in range(epochs):\n","    # Transforma a entrada para uma dimensão\n","    inputs = Variable(x_train.view(-1, 28 * 28))\n","    # Predict da rede\n","    outputs = model(inputs)\n","\n","    # z0 a z9\n","    zs.append(outputs[1].detach().numpy())\n","\n","    # calcula a perda\n","    loss = criterion(outputs, Variable(y_train))\n","\n","    # zero, backpropagation, ajusta parâmetros pelo gradiente descendente\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    losses.append(loss.item())\n","\n","    _, predicts = torch.max(outputs, 1)\n","\n","    y_pred = predicts.data\n","    accuracy = (y_pred.numpy() == y_train.numpy()).mean()\n","\n","    print('Epoch[{}/{}], loss: {:.6f}, acc: {:.6f}'\n","              .format(i+1, epochs, loss.data[0], accuracy))\n","\n","    weights = model.state_dict()['weight']\n","    print('weights:', weights.shape)\n","\n","    bias = model.state_dict()['bias']\n","    print('bias:   ', bias.shape)\n","\n","    # Visualizando pesos da classe 3\n","    plt.imshow(weights[4, :].numpy().reshape((28,28)),cmap = 'gray')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-11-24T15:44:51.685301","start_time":"2017-11-24T15:44:51.680419"},"id":"menkKvteOl27","executionInfo":{"status":"aborted","timestamp":1732828438168,"user_tz":180,"elapsed":10,"user":{"displayName":"CAIO CÉSAR SILVA GUEDES","userId":"04767638617673838831"}}},"source":["print('Final loss:', loss.item())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j1pLKxlqOl3F"},"source":["### Visualizando gráfico de perda durante o treinamento"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-11-24T15:44:56.787741","start_time":"2017-11-24T15:44:56.627754"},"id":"UlivLNgcOl3G","executionInfo":{"status":"aborted","timestamp":1732828438168,"user_tz":180,"elapsed":10,"user":{"displayName":"CAIO CÉSAR SILVA GUEDES","userId":"04767638617673838831"}}},"source":["plt.plot(losses)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zUTfdh5VOl3N"},"source":["## Avaliação"]},{"cell_type":"markdown","metadata":{"id":"XgFPZ9hNOl3O"},"source":["### Acurácia tanto no conjunto de treinamento como no conjunto de testes"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-11-24T15:50:29.922115","start_time":"2017-11-24T15:50:29.914004"},"id":"JMo5hSBhOl3P","executionInfo":{"status":"aborted","timestamp":1732828438168,"user_tz":180,"elapsed":10,"user":{"displayName":"CAIO CÉSAR SILVA GUEDES","userId":"04767638617673838831"}}},"source":["def predict(model, input_data):\n","    outputs = model(Variable(input_data))\n","    _, predicts = torch.max(outputs, 1)\n","\n","    return predicts.data\n","\n","y_pred = predict(model, x_train.view(-1, 28*28))\n","accuracy = (y_pred.numpy() == y_train.numpy()).mean()\n","print('Accuracy:', accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Po0mlGpCOl3W"},"source":["### Matriz de confusão com dados de treinamento e teste"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-11-24T15:51:40.706177","start_time":"2017-11-24T15:51:40.679474"},"id":"PtA3ZAUjOl3X","executionInfo":{"status":"aborted","timestamp":1732828438168,"user_tz":180,"elapsed":10,"user":{"displayName":"CAIO CÉSAR SILVA GUEDES","userId":"04767638617673838831"}}},"source":["print('Matriz de confusão:')\n","pd.crosstab(y_pred.numpy(), y_train.numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Df5Xz6rOl3c"},"source":["## Visualizando a matriz de pesos treinados"]},{"cell_type":"markdown","metadata":{"id":"DLo6HaWGOl3n"},"source":["Observe que a matriz de peso treinado para cada classe mostra a importância dos pesos associados aos caracteres de cada classe."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-11-24T16:34:49.367135","start_time":"2017-11-24T16:34:49.204452"},"id":"uoBSV4EqOl3q","executionInfo":{"status":"aborted","timestamp":1732828438168,"user_tz":180,"elapsed":9,"user":{"displayName":"CAIO CÉSAR SILVA GUEDES","userId":"04767638617673838831"}}},"source":["weights = model.state_dict()['weight']\n","print('weights:', weights.shape)\n","\n","bias = model.state_dict()['bias']\n","print('bias:   ', bias.shape)\n","\n","# Visualizando pesos da classe 3\n","plt.imshow(weights[3, :].numpy().reshape((28,28)),cmap = 'gray')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eN6W4vnfOl3w"},"source":["### Visualizando os pesos de todas as classes"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-11-24T16:34:50.240218","start_time":"2017-11-24T16:34:50.025515"},"id":"0Vz_WwiDOl3x","executionInfo":{"status":"aborted","timestamp":1732828438169,"user_tz":180,"elapsed":10,"user":{"displayName":"CAIO CÉSAR SILVA GUEDES","userId":"04767638617673838831"}}},"source":["# cria um grid com as imagens\n","grid = torchvision.utils.make_grid(weights.view(-1, 1, 28, 28), normalize=True, pad_value=1.0, padding=1, nrow=10)\n","\n","plt.figure(figsize=(15, 10))\n","plt.imshow(grid.numpy().transpose(1, 2, 0))\n","plt.axis('off');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IAwjpjUzOl3z"},"source":["### Diagrama da regressão softmax com visualização dos pesos W"]},{"cell_type":"markdown","metadata":{"id":"Io0kz0bDOl31"},"source":["![alt text](https://raw.githubusercontent.com/vcasadei/images/master/RegressaoSoftmaxArgmaxNMIST.png)"]},{"cell_type":"markdown","metadata":{"id":"KKKZDmIvOl31"},"source":["# Atividades"]},{"cell_type":"markdown","metadata":{"id":"fMcK8rnnOl31"},"source":["## Exercícios"]},{"cell_type":"markdown","metadata":{"id":"FSgqCTaWOl32"},"source":["- 1) Na configuração da figura acima, mostre os valores de z0 até z9, os valores das probabilidades y_hat, após o softmax, quando a rede recebe como entrada a nona amostra que contém o manuscrito do dígito '4':"]},{"cell_type":"markdown","metadata":{"id":"bcYvj-5dOl4I"},"source":["- 2) Insira código no laço do treinamento para que no final de cada época,\n","     seja impresso: o número da época e a perda e a acurácia"]},{"cell_type":"markdown","metadata":{"id":"0xttWrUbOl4J"},"source":["- 3) Insira código no laço do treinamento para visualização dos valores dos gradientes referentes à classe do dígito 4, no final de cada época."]},{"cell_type":"markdown","metadata":{"id":"yDhND6p3Ol4J"},"source":["## Perguntas"]},{"cell_type":"markdown","metadata":{"id":"AXg8GBrsOl4L"},"source":["1. Qual é o shape da matriz de entrada na rede?\n","2. Qual é o shape da saída da rede?\n","3. Qual é o número total de parâmetros da rede, incluindo o bias?"]},{"cell_type":"markdown","metadata":{"id":"0aj1pg_ZOl4L"},"source":["# Aprendizados\n"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"M2JvX4JfOl4M"},"source":["1. Os atributos são os pixels rasterizados (28x28) para entrada na rede neural\n","2. Formato de imagem pode ser canal primeiro ou canal último (c,H,W) ou (H,W,c) respectivamente. O PyTorch utiliza formato (amostras, c, H, W). O matplotlib imshow utiliza formato (H,W,c) ou (H,W)\n","3. Este exemplo permite uma interpretação visual dos pesos (parâmetros treinados)"]}]}